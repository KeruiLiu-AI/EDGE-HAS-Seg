We thank the reviewers for their constructive feedback. The source code is provided for reproducibility: https://github.com/KeruiLiu-AI/EDGE-HAS-Seg

6631: 
1. Fig. 3 already visualizes the reduction in false negatives (blue regions) and boundary improvements compared to H2ASeg. We will highlight the specific areas using boxes to further validate the structural innovation.
2. Given the modular design (DSSM/EGM/DEM), these acronyms are essential for precision and are defined systematically in Sec. 2 to maintain flow. 
3. Our contribution is the specific hierarchical integration of edge and semantic streams, which is fundamentally distinct from the simple concatenation methods cited. 


24B4: 
1. We confirm that all baselines were trained from scratch using official implementations under the identical settings described in Sec. 3.1 to ensure a fair comparison. 
2. K-fold cross-validation is a good method to mitigate the randomness. However, in order to make a more accurate comparison, we followed the practices of H2ASeg and averaged four independent experiments.
3. The lack of standard deviation data was our oversight. We will supplement the data in Table 1. Some data are as follows.

H2ASeg: Dice 62.14±14.75, HD95 19.91±30.44, Pre 74.97±16.03, Rec 60.47±19.87

Ours: Dice 71.75±14.29, HD95 15.58±30.15, Pre 70.44±16.00, Rec 81.06±19.54

3DFD: 
1. We admit that using $D$ for both the "Depth" dimension and "Edge" features was a mistake.  We will rename the depth dimension variable from $D$ to $Z$. D_l^{pet/ct} are introduced with a clear definition or source in Sec. 2.3. We further mark the symbols on the feature flows in the figure to present a clearer definition and source.


2. We apologize for the simplification in the text. Our code implements a 1x1x1 projection convolution after concatenation to align channels. We correct Eq. 7, 8, and 12. For example, R=reshape(Linear(reshape(X||D))),G=GELU(Conv_{1x1x1}(R + X)), E=Q+R. We correct Fig. 2 to show that the starting point of the second residual connection R is after the linear layer. Our implementation uses a multi-channel Sobel operator, where edge filters are applied distinctively to feature channels. The 'single-channel' description in the text was imprecise; we meant that it extracts structural information per channel. We will correct the text to reflect the multi-channel nature of the edge guidance.
 3. The EGM is not a reuse of the main encoder; it is a separate, parallel encoder stream with independent weights. The PET and CT branches are separate streams with independent weights (Pseudo-Siamese structure). This allows the network to learn modality-specific features without interference.  We do not simply add features. In DEM, we utilize theLarge Kernel Attention and DEAB module. This module dynamically computes attention weights for PET and CT features separately before fusing them. This acts as a learnable, modality-aware alignment mechanism that emphasizes relevant features from each modality.
4.  While we used a single benchmark, AutoPET-II is one of the largest and most diverse public datasets available for whole-body PET/CT (1014 scans). The proposed DEAB is critical for effectively fusing the parallel streams. We have conducted ablation studies (Section 3.3). Specifically, replacing DEAB with simple concatenation leads to a performance drop (Dice decreases by 3.81%), proving that DEAB effectively filters noise and aligns cross-modal features. We will expand the discussion in the "Ablation Study" section to provide a more detailed breakdown of these results.


4606: 
1. In our EGM design (specifically the EGAB module, see source code EGM.py), the Sobel operator acts only as an initial attention mechanism, not the final feature descriptor. Learnable Filtering: The Sobel-guided features are immediately followed by Batch Normalization (BN) and learnable $3\times3\times3$ Convolutions (self.conv1 in EGM.py). These layers function as learnable denoisers, allowing the network to distinguish between consistent structural edges and random PET noise during training. Soft Attention: We usea  sigmoid to generate a soft attention map, which weighs features rather than hard-coding edges. 
 2. The "spatial mismatch" (PET hypermetabolic region < CT anatomical boundary) is actually a key feature we exploit, serving as a Semantic Gating mechanism. Feature Filtering: CT images contain abundant anatomical edges (bones, organs) that are irrelevant to tumors. The Hadamard product with PET features effectively suppresses these non-tumor anatomical boundaries, acting as a "tumor-region selector."Information Preservation: Crucially, as shown in Eq. (6) and our code (DEM.py), we employ Residual Connections (+ S_{origin}^l) alongside the fusion. This ensures that essential anatomical cues from CT are not permanently lost if the PET signal is weak, preventing the "suppression" issue the reviewer is concerned about.
3. We intentionally excluded a specific boundary loss for two strategic reasons: Fair Comparison & Focus on Encoder: As stated in the paper, our goal was to validate the effectiveness of the proposed feature extraction modules (DSSM, EGM, DEM). By strictly adhering to the decoder architecture and hybrid loss function (BCE + Dice) of the H2ASeg framework [15], we ensure a controlled experimental setting. This isolates the source of performance gains: the significant improvement in HD95 (from 19.91mm in H2ASeg to 15.58mm in ours) is solely attributable to our novel encoder design and feature fusion strategy, rather than hyper-parameter tuning of auxiliary losses. Architectural Inductive Bias: We believe that structural guidance should be intrinsic to the network. The EGM (Edge-Guided Module) explicitly encodes boundary information into the feature space using learnable Sobel operators. This acts as a strong inductive bias, enabling the network to learn edge-aware representations naturally during backpropagation, rendering additional boundary loss terms redundant.


4. The dramatic drop in HD95 is attributed to a two-stage improvement: Elimination of Outliers (Global): The majority of the drop (from 40.18mm) is indeed due to the model eliminating distant False Positives (FPs). In single-modality or simple fusion methods, high-uptake organs (e.g., bladder, heart) in PET or similar-density tissues in CT often cause distant FPs. Our DEM (Dual Enhancement Module) effectively fuses the two, using the intersection of "High Uptake" AND "Anatomical Structure" to filter out these outliers. Boundary Refinement (Local): Once outliers are removed, the EGM refines the contours of the actual tumor, pushing the metric further down to 15.58mm. This is supported by the visual results in Fig. 3 (Columns 3 & 4), where our method shows significantly tighter boundary adherence (less blue/false negative areas) compared to H2ASeg.
 
5. We acknowledge the increased complexity but emphasize the high efficiency-to-performance ratio. Comparison: While our parameter count is higher than the single-stream UNet due to the dual-branch design, it is comparable to other multi-modal methods like H2ASeg.Inference Speed: Since our model is fully CNN-based (avoiding the heavy self-attention computation of Transformers like SwinUNETR), the inference remains efficient. On a single RTX 3090, our model processes a 128x128x64 volume in approximately 0.15s, which satisfies clinical real-time requirements. We will include a detailed Params/FLOPs/FPS comparison table in the final version.

